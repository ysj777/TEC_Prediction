[global]
seed = 13
predict_range = global
; global or single point (Longitude, latitude)
device = cuda
; cuda
; cpu

[train]
epoch = 80
batch_size = 32
lr = 1e-5
shuffle = True
num_worker = 0
criterion = MSELoss
optimizer = AdamW
; SGD
; AdamW
lr_scheduler = ReduceLROnPlateau
; CosineAnnealingLR
; ReduceLROnPlateau
; OneCycleLR
teacher_forcing_ratio = 0.5

[eval]
batch_size = 8
shuffle = False
num_worker = 0

[preprocess]
normalization_type = min_max
; min_max
; z_score
; None
predict_norm = False
; auto False if normalization_type = None

[data]

train_year = 2018, 2019
test_year = 2020, 2021
valid_ratio = 0.8

dataset_type = SWGIMDataset
date_features = DOY, hour
; year, DOY, hour
global_features = kp, r, dst, ap, f10.7, storm_state, storm_size
; kp, r, dst, ap, f10.7, storm_state, storm_size;
seq_feature = True
; True then add position of the input sequence
tec_features  = tec
; tec or tec_sh
seq_base = longitude
; time or latitude, longitude
; tec_sh seq_base only type time

reduce = False
reduce_ratio = 0.2

date_seq_base_norm = Hibert
; Hibert
; None

[model]
model_name = Transformer_E
; LSTM_TEC
; LSTM_Seq2Seq_TEC
; Transformer_E
; Transformer_ED
input_time_step = 24
; model input x hours TEC
output_time_step = 1
; model output x-hour-later TEC
embedding_size = 512
; ignore in Transformer (equal to hidden_size)
; for seq2seq model
hidden_size = 128
num_layer = 12
dropout = 0.5

[output]
output_func = SWGIM
rounding_digit = 5